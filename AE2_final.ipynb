{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "from flask import Flask, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wikipedia():\n",
    "    # Initialize Wikipedia API with a custom user agent\n",
    "    # This is important to identify the crawler to Wikipedia servers\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en', \n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "        user_agent='MyCustomWikipediaCrawler/1.0 (https://mywebsite.com)'\n",
    "    )\n",
    "\n",
    "    # List to store all the page contents\n",
    "    all_pages = []\n",
    "    \n",
    "    # Categories we want to crawl\n",
    "    categories = [\"Category:History\", \"Category:Science\", \"Category:Art\", \"Category:Technology\"]\n",
    "\n",
    "    # Iterate over each category\n",
    "    for category in categories:\n",
    "        # Get the page object for the category\n",
    "        cat = wiki_wiki.page(category)\n",
    "        # Extract all members (pages) of the category\n",
    "        pages = cat.categorymembers.values()\n",
    "\n",
    "        # Iterate over each page in the category\n",
    "        for page in pages:\n",
    "            # Check if the page is an article (namespace = MAIN)\n",
    "            if page.ns == wikipediaapi.Namespace.MAIN:\n",
    "                # Append the text of the page to the list\n",
    "                all_pages.append(page.text)\n",
    "                # Break if we have collected 100 pages\n",
    "                if len(all_pages) >= 100:\n",
    "                    break\n",
    "\n",
    "        # Break the outer loop if we have collected 100 pages\n",
    "        if len(all_pages) >= 100:\n",
    "            break\n",
    "\n",
    "    # Return the list of page contents\n",
    "    return all_pages\n",
    "\n",
    "# Execute the crawl\n",
    "data = crawl_wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters and punctuation\n",
    "    words = word_tokenize(text)  # Tokenize into words\n",
    "    words = [word for word in words if word.isalpha()]  # Keep only alphabetic words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess all pages\n",
    "preprocessed_data = [preprocess_text(page) for page in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each preprocessed page into sentences\n",
    "tokenized_data = [word_tokenize(page) for page in preprocessed_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=500, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_wikipedia.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "model.build_vocab(tokenized_data, update=False)\n",
    "model.train(tokenized_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('word2vec_wikipedia.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"word2vec_wikipedia.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_pair = (\"science\", \"history\")\n",
    "target_word = \"ancient\"\n",
    "result_vector = model.wv[target_word] - model.wv[reference_pair[0]] + model.wv[reference_pair[1]]\n",
    "opposite_words = model.wv.similar_by_vector(result_vector)\n",
    "print(opposite_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "html_form_with_message = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Text Echo App</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Enter Text</h2>\n",
    "    <form method=\"post\" action=\"/\">\n",
    "        <label for=\"text\">Text:</label><br>\n",
    "        <input type=\"text\" name=\"my_input_value\"><br><br>\n",
    "        <input type=\"submit\" value=\"My Button\">\n",
    "    </form>\n",
    "    <p>The opposite is: put_data_here</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "def my_input(value):\n",
    "    reference_pair = (\"science\", \"history\")\n",
    "    result_vector = model.wv[value] - model.wv[reference_pair[0]] + model.wv[reference_pair[1]]\n",
    "    opposite_words = model.wv.similar_by_vector(result_vector)\n",
    "    return(opposite_words[0][0])\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    user_input = ''\n",
    "    opposite_input = ''\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['my_input_value']\n",
    "        opposite_input = my_input(user_input)\n",
    "    return html_form_with_message.replace(\"put_data_here\", opposite_input)\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Library Imports and Initial Setup:**\n",
    "   - Importing essential libraries. `wikipediaapi` is used for accessing and retrieving data from Wikipedia, while `nltk` (Natural Language Toolkit) and `gensim` are crucial for natural language processing and machine learning tasks respectively.\n",
    "   - The script also includes the downloading of 'punkt', a pre-trained tokenizer model from the NLTK library, essential for text tokenization tasks.\n",
    "\n",
    "2. **Function to Crawl Wikipedia Pages:**\n",
    "   - A custom function named `crawl_wikipedia` is defined to systematically scrape content from Wikipedia. \n",
    "   - The function sets up the Wikipedia API with specific parameters like the desired language (English) and a custom user agent for identification purposes.\n",
    "   - The scraper targets four specific categories: History, Science, Art, and Technology. It iterates through each category, extracting the main content from each page.\n",
    "   - A limit is set to stop the crawling process after accumulating content from 100 pages, balancing the need for a sizable dataset with the practicality of processing time and resource allocation.\n",
    "\n",
    "3. **Execution of Web Scraping:**\n",
    "   - This part of the notebook is where the `crawl_wikipedia` function is called into action. The output is a collection of text data from the targeted Wikipedia pages.\n",
    "\n",
    "4. **Text Preprocessing Function:**\n",
    "   - Introduces a function `preprocess_text` to clean and prepare the scraped text for analysis. \n",
    "   - This preprocessing includes converting all text to lowercase (to ensure uniformity), removing special characters and punctuation (to focus on actual words), and tokenizing the text into individual words.\n",
    "   - The function filters out tokens that are not purely alphabetic, thereby removing numbers and any remaining special characters.\n",
    "\n",
    "5. **Tokenization of Preprocessed Data:**\n",
    "   - Proceeds to tokenize the preprocessed text data into words. This step is crucial for word embedding models, which require word-level inputs.\n",
    "\n",
    "6. **Training the Word2Vec Model:**\n",
    "   - A Word2Vec model from the `gensim` library is trained on the tokenized text. \n",
    "   - The model parameters include a vector size of 500, which determines the dimensionality of the word vectors, and a minimum count of 1 for words to be included in the model training. \n",
    "   - The training utilizes 4 worker threads, indicating a multi-threaded approach to speed up the training process.\n",
    "\n",
    "7. **Saving the Word2Vec Model:**\n",
    "   - The trained Word2Vec model is saved to a file, allowing for its reuse without the need to retrain it.\n",
    "\n",
    "8. **Displaying Tokenized Data:**\n",
    "   - This part of the notebook intended for displaying the tokenized data, possibly for verification or review purposes.\n",
    "\n",
    "9. **Model Vocabulary Building and Retraining:**\n",
    "   - The script rebuilds the model's vocabulary with the tokenized data and retrains the Word2Vec model.\n",
    "   - The retrained model is then saved, presumably to update it with any new data or adjustments.\n",
    "\n",
    "10. **Loading the Trained Model:**\n",
    "    - The trained and saved Word2Vec model is loaded from the file for further use.\n",
    "\n",
    "11. **Word Vector Arithmetic and Similarity Calculation:**\n",
    "    - Demonstrates an application of the trained Word2Vec model. \n",
    "    - It performs vector arithmetic using words like \"science,\" \"history,\" and \"ancient\" to find semantically similar words in a shifted context.\n",
    "    - The idea is to explore how the meaning of \"ancient\" changes when the context moves from \"science\" to \"history\".\n",
    "    - The results of this vector arithmetic are printed, showcasing the model's capability to understand and manipulate word meanings based on their vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
